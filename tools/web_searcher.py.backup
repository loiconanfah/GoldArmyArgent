"""Outil de recherche web robuste (Job Bank Scraper + Fallback)."""
import urllib.request
import urllib.parse
import ssl
import asyncio
from typing import List, Dict, Any
import re
from loguru import logger

# D√©sactiver v√©rification SSL pour simplifier
ssl_context = ssl._create_unverified_context()

class JobWebSearcher:
    """Recherche web sp√©cialis√©e pour l'emploi (Priorit√©: Guichet Emplois/Job Bank)."""
    
    async def search_jobs(self, keywords: str, location: str = "Qu√©bec", job_type: str = "stage", max_results: int = 20) -> List[Dict[str, Any]]:
        import asyncio
        logger.info(f"üåê Recherche Web Parall√®le: '{keywords}' √† '{location}'")
        
        all_jobs = []
        
        # Lancement parall√®le des recherches
        # On lance JobBank et DDG en m√™me temps pour la vitesse
        tasks = [
            self._search_jobbank(keywords, location, max_results),
            self._search_general_web(keywords, location, max_results)
        ]
        
        # Playwright est d√©sactiv√© le temps de r√©gler le probl√®me AV, ou on pourrait l'ajouter ici
        # tasks.append(self._search_with_playwright(...))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for res in results:
            if isinstance(res, list):
                all_jobs.extend(res)
            elif isinstance(res, Exception):
                logger.warning(f"‚ö†Ô∏è Une source de recherche a √©chou√©: {res}")
                
        # Deduplication simple par URL ou Titre+Compagnie
        unique_jobs = []
        seen_urls = set()
        
        for job in all_jobs:
            url = job.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_jobs.append(job)
            elif not url:
                unique_jobs.append(job) # Garder si pas d'url (rare)
                
        all_jobs = unique_jobs[:max_results*2] # Garder un peu de marge
            
        # Fallback si rien trouv√©
        if not all_jobs:
            logger.warning("‚ö†Ô∏è Aucun r√©sultat direct -> G√©n√©ration de liens de recherche")
            all_jobs = self._generate_fallback_links(keywords, location)
            
        logger.success(f"‚úÖ {len(all_jobs)} offres trouv√©es au total (recherche parall√®le).")
        return all_jobs

    async def enrich_job_details(self, job: Dict[str, Any]) -> Dict[str, Any]:
        """Enrichit une offre avec les d√©tails complets de la page."""
        if job.get("source") != "Guichet Emplois":
            return job # Support limit√© pour l'instant
            
        url = job.get("url")
        if not url: return job
            
        logger.debug(f"üìÑ Analyse approfondie: {job['title']}")
        try:
            req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urllib.request.urlopen(req, context=ssl_context, timeout=5) as response:
                html = response.read().decode('utf-8')
                
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')
                
                # Extraction description Job Bank (Plusieurs s√©lecteurs possibles)
                desc_text = ""
                
                # 1. Conteneur principal visuel
                desc_div = soup.find(id="job-posting-details-body")
                if desc_div:
                    desc_text = desc_div.get_text(separator="\n", strip=True)
                
                # 2. Fallback: M√©tadonn√©e SEO (souvent pr√©sente et propre)
                if not desc_text:
                    meta_desc = soup.find("span", property="description")
                    if meta_desc:
                        desc_text = meta_desc.get_text(separator="\n", strip=True)
                
                if desc_text:
                    job["full_description"] = desc_text
                    
                    skills = []
                    
                    # Cas A: On a un conteneur structur√©
                    if desc_div:
                        for ul in desc_div.find_all('ul'):
                            for li in ul.find_all('li'):
                                text = li.get_text(strip=True)
                                if len(text) < 100: # √âviter paragraphes
                                    skills.append(text)
                    
                    # Cas B: On a juste le texte (via meta description)
                    else:
                        # Essayer de trouver des lignes commen√ßant par * ou -
                        for line in desc_text.split('\n'):
                            line = line.strip()
                            if line.startswith(('*', '-', '‚Ä¢')):
                                clean_line = line.lstrip('* -‚Ä¢').strip()
                                if 3 < len(clean_line) < 100:
                                    skills.append(clean_line)
                    
                    if skills:
                        job["required_skills"] = list(set(skills))[:15] # Top 15 uniques
                        
                return job
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec enrichissement {url}: {e}")
            return job

    async def scrape_page_content(self, url: str) -> str:
        """T√©l√©charge et extrait le texte principal d'une page web."""
        try:
            # Filtrer les fichiers non-HTML
            if url.lower().endswith(('.pdf', '.doc', '.docx')):
                return ""

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            
            # Utilisation de aiohttp pour l'async si possible, sinon urllib (synchrone mais robuste ici dans threadpool)
            # Pour simplifier et rester compatible avec le reste du code synchrone/async mix
            loop = asyncio.get_event_loop()
            
            def _fetch():
                req = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(req, context=ssl_context, timeout=10) as response:
                    return response.read().decode('utf-8', errors='ignore')

            html = await loop.run_in_executor(None, _fetch)
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            # Suppression des √©l√©ments inutiles
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
                tag.decompose()
            
            # Extraction du texte
            text = soup.get_text(separator=' ', strip=True)
            
            # Nettoyage
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            clean_text = ' '.join(lines)
            
            # Limite de taille pour le LLM (env. 2000 mots / 8k chars)
            return clean_text[:8000]

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Scraping √©chou√© pour {url}: {e}")
            return ""

    async def _search_jobbank(self, keywords: str, location: str, max_results: int) -> List[Dict[str, Any]]:
        """Scrape les r√©sultats HTML de Job Bank (Guichet Emplois)."""
        logger.debug(f"üîç Scraping Job Bank pour: {keywords} {location}")
        
        # Gestion pr√©cise de la localisation pour √©viter Toronto/Halifax quand on veut Qu√©bec
        loc_param = location
        strict_city_filter = None
        
        if location.lower() in ["qu√©bec", "quebec"]:
            loc_param = "Qu√©bec, QC" 
            strict_city_filter = ["qu√©bec", "quebec", "l√©vis", "levis", "sainte-foy"]
        elif location.lower() in ["montr√©al", "montreal"]:
            loc_param = "Montr√©al, QC"
            strict_city_filter = ["montr√©al", "montreal", "laval", "longueuil"]
            
        base_url = "https://www.jobbank.gc.ca/jobsearch/jobsearch"
        # Nettoyage basique des mots-cl√©s : retirer la location si pr√©sente
        # Ex: "boulanger √† Qu√©bec" -> "boulanger" (car location est d√©j√† g√©r√© par param)
        clean_keywords = keywords.lower().replace(f" √† {location.lower()}", "").replace(f" in {location.lower()}", "").strip()
        if location.lower() in clean_keywords:
             clean_keywords = clean_keywords.replace(location.lower(), "").strip()
        
        params = {
            "searchstring": clean_keywords if clean_keywords else keywords,
            "locationstring": loc_param,
            "sort": "M" 
        }
        url = f"{base_url}?{urllib.parse.urlencode(params)}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        jobs = []
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, context=ssl_context, timeout=10) as response:
                html = response.read().decode('utf-8')
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')
                
                # Scraper tous les articles
                articles = soup.find_all('article')
                logger.debug(f"üîç Articles trouv√©s dans le HTML: {len(articles)}")
                
                for i, article in enumerate(articles): 
                    if len(jobs) >= max_results: break
                    
                    link_tag = article.find('a', class_='resultJobItem')
                    if not link_tag: continue
                    
                    title = article.find(class_='noctitle').get_text(strip=True)
                    href = link_tag.get('href', '')
                    full_url = f"https://www.jobbank.gc.ca{href}"
                    
                    # Extraction Compagnie
                    company = "Entreprise confidentielle"
                    business_tag = article.find(class_='business')
                    if business_tag: company = business_tag.get_text(strip=True)
                        
                    # Extraction Lieu
                    loc_text = location
                    loc_tag = article.find(class_='location')
                    if loc_tag: 
                        loc_text = " ".join(loc_tag.get_text().split())
                    
                    # FILTRAGE STRICT ACTIV√â
                    # Si on cherche √† Qu√©bec, on ne veut pas Toronto
                    if strict_city_filter:
                        is_valid_loc = any(city in loc_text.lower() for city in strict_city_filter)
                        if not is_valid_loc:
                            # logger.debug(f"üõë Ignored location: '{loc_text}'")
                            continue
                    
                    date_text = ""
                    date_tag = article.find(class_='date')

                    date_text = ""
                    date_tag = article.find(class_='date')
                    if date_tag: date_text = date_tag.get_text(strip=True)
                        
                    job = {
                        "id": f"jb-{i}",
                        "title": title,
                        "company": company,
                        "location": loc_text,
                        "url": full_url,
                        "source": "Guichet Emplois",
                        "description": f"{title} chez {company}. {loc_text}. Publi√©: {date_text}",
                        "required_skills": [keywords], 
                        "match_score": 0
                    }
                    jobs.append(job)
        except Exception as e:
            logger.error(f"‚ùå Erreur Job Bank: {e}")
            
        return jobs

    async def _search_general_web(self, keywords: str, location: str, max_results: int) -> List[Dict[str, Any]]:
        """
        Recherche via Google HTML avec scraping individuel des offres.
        √âvite les liens de recherche g√©n√©raux.
        """
        logger.info(f"üîé Recherche Google HTML: {keywords} √† {location}")
        jobs = []
        
        # Requ√™tes cibl√©es pour trouver des offres sp√©cifiques
        # On cherche des pages d'offres individuelles, pas des pages de recherche
        sites = [
            ("Indeed", f'site:ca.indeed.com/rc/clk "{keywords}" "{location}"'),
            ("LinkedIn", f'site:ca.linkedin.com/jobs/view "{keywords}" "{location}"'), 
            ("JobBank", f'site:jobbank.gc.ca/job "{keywords}" "{location}"')
        ]
                    # Filtrer les URLs de recherche g√©n√©raux
                    if any(path in href.lower() for path in ['/jobs?', '/search?', '/jobsearch?', '/jobs/search', '/j2jk']):
                        logger.debug(f"  Ignoring search page: {href}")
                        continue
                    
                    # Nettoyage du titre
                    clean_title = title
                    if " - " in title:
                        parts = title.split(" - ")
                        clean_title = parts[0]
                    
                    # Extraction du nom de l'entreprise depuis le snippet
                    company = source_name
                    if " chez " in snippet.lower():
                        parts = snippet.lower().split(" chez ")
                        if len(parts) > 1:
                            company_part = parts[1].split()[0] if parts[1].split() else ""
                            if company_part:
                                company = company_part.capitalize()
                    
                    job = {
                        "id": f"gg-{source_name}-{i}",
                        "title": clean_title,
                        "company": company,
                        "location": location,
                        "description": snippet,
                        "url": href,
                        "source": source_name,
                        "required_skills": [],
                        "match_score": 0,
                        "scraped": False
                    }
                    found_jobs.append(job)
                    
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Erreur Google Scraper ({source_name}): {e}")
            
            return found_jobs

        # Ex√©cution parall√®le
        tasks = []
        for src, q in sites:
            tasks.append(loop.run_in_executor(None, _scrape_google, src, q))
            
        results_list = await asyncio.gather(*tasks)
        
        for src_res in results_list:
            jobs.extend(src_res)
            
        return jobs

    async def _search_with_playwright(self, keywords: str, location: str, max_results: int) -> List[Dict[str, Any]]:
        """
        Recherche avanc√©e via navigation r√©elle (Playwright).
        Contourne les limitations JS et Captchas simples.
        """
        from playwright.async_api import async_playwright
        jobs = []
        
        logger.info(f"üé≠ Lancement du navigateur pour : {keywords} √† {location}")
        
        try:
            async with async_playwright() as p:
                # Tentative avec Chromium + Headful (Visible) pour √©viter blocage AV
                browser = await p.chromium.launch(headless=False) 
                page = await browser.new_page()
                
                # Simulation d'un utilisateur r√©el
                await page.set_extra_http_headers({
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                })
                
                # Strat√©gie 1: Google Jobs
                await page.goto(f"https://www.google.com/search?q={keywords} jobs {location}")
                
                try:
                    await page.wait_for_selector("div.g", timeout=5000)
                except:
                    logger.warning("Timeout waiting for Google results")
                
                results = await page.query_selector_all("div.g")
                
                for res in results:
                    if len(jobs) >= max_results: break
                    
                    try:
                        title_el = await res.query_selector("h3")
                        link_el = await res.query_selector("a")
                        snippet_el = await res.query_selector("div.VwiC3b") 
                        
                        if title_el and link_el:
                            title = await title_el.inner_text()
                            url = await link_el.get_attribute("href")
                            snippet = await snippet_el.inner_text() if snippet_el else ""
                            
                            if not url or "google" in url: continue
                            
                            source = "Web"
                            if "indeed.com" in url: source = "Indeed"
                            elif "linkedin.com" in url: source = "LinkedIn"
                            elif "glassdoor" in url: source = "Glassdoor"
                            
                            jobs.append({
                                "id": f"pw-{len(jobs)}",
                                "title": title,
                                "company": source,
                                "location": location,
                                "url": url,
                                "source": source,
                                "description": snippet,
                                "required_skills": [keywords],
                                "match_score": 0
                            })
                            logger.info(f"   üîπ Trouv√© (PW): {title} ({source})")
                            
                    except Exception:
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Playwright: {e}")
            
        return jobs

    def _generate_fallback_links(self, keywords: str, location: str) -> List[Dict[str, Any]]:
        """G√©n√®re des liens de recherche directs."""
        params = urllib.parse.quote_plus(f"{keywords} {location}")
        k_enc = urllib.parse.quote_plus(keywords)
        l_enc = urllib.parse.quote_plus(location)
        
        return [
            {
                "id": "search-guichet",
                "title": f"Voir les offres '{keywords}' sur Guichet Emplois",
                "company": "Gouvernement du Canada",
                "location": location,
                "url": f"https://www.guichetemplois.gc.ca/jobsearch/jobsearch?searchstring={k_enc}&locationstring={l_enc}",
                "source": "Search Link",
                "description": "Source officielle. Cliquez pour voir les r√©sultats d√©taill√©s.",
                "required_skills": ["Navigation Web"],
                "match_score": 85
            },
            {
                "id": "search-indeed",
                "title": f"Voir les offres '{keywords}' sur Indeed",
                "company": "Indeed",
                "location": location,
                "url": f"https://ca.indeed.com/jobs?q={k_enc}&l={l_enc}",
                "source": "Search Link",
                "description": "Agr√©gateur d'emplois populaire.",
                "required_skills": ["Navigation Web"],
                "match_score": 80
            },
            {
                "id": "search-linkedin",
                "title": f"Voir les offres '{keywords}' sur LinkedIn",
                "company": "LinkedIn",
                "location": location,
                "url": f"https://www.linkedin.com/jobs/search?keywords={k_enc}&location={l_enc}",
                "source": "Search Link",
                "description": "R√©seau professionnel.",
                "required_skills": ["Navigation Web"],
                "match_score": 75
            }
        ]

# Instance globale
web_searcher = JobWebSearcher()

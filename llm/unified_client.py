
"""Client Unifi√© pour la gestion des mod√®les LLM (OpenRouter + Fallback Local)."""
import asyncio
from typing import Optional, Dict, List, Any
from loguru import logger

from llm.ollama_client import OllamaClient
from llm.openrouter_client import OpenRouterClient
from config.settings import settings


class UnifiedLLMClient:
    """
    Client centralis√© qui g√®re la strat√©gie de s√©lection de mod√®le.
    Strat√©gie:
    1. Tenter OpenRouter (si cl√© API pr√©sente).
    2. Si √©chec ou pas de cl√© -> Fallback sur Ollama Local.
    """
    
    def __init__(self):
        self.ollama_client = OllamaClient()
        self.openrouter_client = None
        self.gemini_client = None
        
        if settings.gemini_api_key:
            from llm.gemini_client import GeminiClient
            self.gemini_client = GeminiClient()
            logger.info("üß† Client Unifi√©: Google Gemini activ√© (Priorit√© Absolue)")
        elif settings.openrouter_api_key:
            self.openrouter_client = OpenRouterClient()
            logger.info("üåê Client Unifi√©: OpenRouter activ√© (Prioritaire)")
        else:
            logger.info("üè† Client Unifi√©: Mode Local uniquement (Ollama)")
            
    async def initialize(self):
        """Initialisation des sous-clients si n√©cessaire."""
        # Rien de sp√©cial √† faire pour l'instant, mais gard√© pour compatibilit√©
        pass

    async def close(self):
        """Ferme les connexions."""
        await self.ollama_client.close()
        if self.openrouter_client:
            await self.openrouter_client.close()
        if self.gemini_client:
            await self.gemini_client.close()

    async def generate(self, prompt: str, **kwargs) -> str:
        """
        G√©n√®re une r√©ponse en utilisant la meilleure strat√©gie disponible.
        """
        # On extrait le mod√®le demand√© pour √©viter les doublons dans kwargs
        requested_model = kwargs.pop("model", None)

        # 0. Essai Gemini
        if self.gemini_client:
            try:
                logger.debug(f"üß† Tentative Gemini Native...")
                return await self.gemini_client.generate(prompt, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec Gemini ({e})... Bascule sur OpenRouter.")

        # 1. Essai OpenRouter
        if self.openrouter_client:
            try:
                # On utilise le mod√®le demand√© ou celui par d√©faut pour OpenRouter
                model = requested_model or settings.openrouter_default_model
                
                logger.debug(f"üåê Tentative OpenRouter avec {model}...")
                return await self.openrouter_client.generate(prompt, model=model, **kwargs)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec OpenRouter ({e})... Bascule sur Ollama Local.")
        
        # 2. Fallback Ollama
        try:
            # Pour le fallback local, on force le mod√®le local par d√©faut
            # car le mod√®le OpenRouter (ex: gpt-4) n'existe probablement pas en local
            model = settings.ollama_default_model
            logger.debug(f"üè† Utilisation Ollama Local avec {model}...")
            
            return await self.ollama_client.generate(prompt, model=model, **kwargs)
            
        except Exception as e:
            logger.error(f"‚ùå √âchec Critique (OpenRouter et Ollama): {e}")
            raise e

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """Mode Chat unifi√© avec support de l'historique complet."""
        # 0. Essai Gemini (a une m√©thode chat() native qui supporte l'historique)
        if self.gemini_client:
            try:
                return await self.gemini_client.chat(messages, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec Gemini Chat ({e}). Bascule sur OpenRouter.")

        # 1. Essai OpenRouter (via generate, en assemblant l'historique dans le prompt)
        if self.openrouter_client:
            try:
                model = kwargs.get("model") or settings.openrouter_default_model
                # Assembler l'historique dans un prompt structur√©
                system_msgs = [m["content"] for m in messages if m["role"] == "system"]
                system = "\n".join(system_msgs) if system_msgs else None
                conv = ""
                for msg in messages:
                    if msg["role"] == "system":
                        continue
                    role_label = "Utilisateur" if msg["role"] == "user" else "Assistant"
                    conv += f"\n{role_label}: {msg['content']}"
                conv = conv.strip()
                return await self.openrouter_client.generate(conv, model=model, system=system, **{k: v for k, v in kwargs.items() if k != "model"})
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec OpenRouter Chat ({e}). Bascule sur Ollama.")

        # 2. Fallback Ollama (a aussi une m√©thode chat native)
        return await self.ollama_client.chat(messages, **kwargs)

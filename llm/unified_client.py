
"""Client Unifi√© pour la gestion des mod√®les LLM (OpenRouter + Fallback Local)."""
import asyncio
from typing import Optional, Dict, List, Any
from loguru import logger

from llm.ollama_client import OllamaClient
from llm.openrouter_client import OpenRouterClient
from config.settings import settings


class UnifiedLLMClient:
    """
    Client centralis√© qui g√®re la strat√©gie de s√©lection de mod√®le.
    Strat√©gie:
    1. Tenter OpenRouter (si cl√© API pr√©sente).
    2. Si √©chec ou pas de cl√© -> Fallback sur Ollama Local.
    """
    
    def __init__(self):
        self.ollama_client = OllamaClient()
        self.openrouter_client = None
        self.gemini_client = None
        
        if settings.gemini_api_key:
            from llm.gemini_client import GeminiClient
            self.gemini_client = GeminiClient()
            logger.info("üß† Client Unifi√©: Google Gemini activ√© (Priorit√© Absolue)")
        elif settings.openrouter_api_key:
            self.openrouter_client = OpenRouterClient()
            logger.info("üåê Client Unifi√©: OpenRouter activ√© (Prioritaire)")
        else:
            logger.info("üè† Client Unifi√©: Mode Local uniquement (Ollama)")
            
    async def initialize(self):
        """Initialisation des sous-clients si n√©cessaire."""
        # Rien de sp√©cial √† faire pour l'instant, mais gard√© pour compatibilit√©
        pass

    async def close(self):
        """Ferme les connexions."""
        await self.ollama_client.close()
        if self.openrouter_client:
            await self.openrouter_client.close()
        if self.gemini_client:
            await self.gemini_client.close()

    async def generate(self, prompt: str, **kwargs) -> str:
        """
        G√©n√®re une r√©ponse en utilisant la meilleure strat√©gie disponible.
        """
        # On extrait le mod√®le demand√© pour √©viter les doublons dans kwargs
        requested_model = kwargs.pop("model", None)

        # 0. Essai Gemini
        if self.gemini_client:
            try:
                logger.debug(f"üß† Tentative Gemini Native...")
                return await self.gemini_client.generate(prompt, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec Gemini ({e})... Bascule sur OpenRouter.")

        # 1. Essai OpenRouter
        if self.openrouter_client:
            try:
                # On utilise le mod√®le demand√© ou celui par d√©faut pour OpenRouter
                model = requested_model or settings.openrouter_default_model
                
                logger.debug(f"üåê Tentative OpenRouter avec {model}...")
                return await self.openrouter_client.generate(prompt, model=model, **kwargs)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec OpenRouter ({e})... Bascule sur Ollama Local.")
        
        # 2. Fallback Ollama
        try:
            # Pour le fallback local, on force le mod√®le local par d√©faut
            # car le mod√®le OpenRouter (ex: gpt-4) n'existe probablement pas en local
            model = settings.ollama_default_model
            logger.debug(f"üè† Utilisation Ollama Local avec {model}...")
            
            return await self.ollama_client.generate(prompt, model=model, **kwargs)
            
        except Exception as e:
            logger.error(f"‚ùå √âchec Critique (OpenRouter et Ollama): {e}")
            raise e

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """Mode Chat unifi√©."""
        # 0. Essai Gemini
        if self.gemini_client:
            try:
                return await self.gemini_client.chat(messages, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec Gemini Chat ({e}).")

        # 1. Essai OpenRouter
        if self.openrouter_client:
            try:
                # OpenRouter utilise /chat/completions qui est similaire √† generate() avec messages
                # Mais notre client OpenRouter a une m√©thode generate() qui g√®re les messages si "system" est pass√©
                # Pour le chat complet, on doit adapter.
                
                # Note: OpenRouterClient.generate g√®re d√©j√† l'assemblage messages -> payload
                # Mais ici on a d√©j√† une liste de messages.
                # On va appeler directement la m√©thode client interne si besoin ou adapter openrouter_client.
                
                # Simplification: OpenRouterClient n'a pas de m√©thode chat() explicite dans mon impl√©mentation pr√©c√©dente ?
                # V√©rifions... J'ai cod√© generate() qui construit le payload chat/completions.
                # Mais il prend "prompt" et "system".
                # Je vais tricher: concat√©ner ou appeler une m√©thode priv√©e si elle existait.
                
                # Mieux: modifier OpenRouterClient pour accepter messages, ou adapter ici.
                # Pour l'instant, on assume que OpenRouterClient.generate est assez flexible ou on l'am√©liore.
                # Hack temporaire: extraire le dernier user message comme prompt.
                
                # CORRECTIF: Je vais plut√¥t appeler _raw_chat sur OpenRouterClient si je l'avais fait, 
                # mais comme je viens de le cr√©er, je sais que generate() prend prompt/system.
                
                # Pour faire propre, je vais utiliser generate() avec le dernier message
                # et concat√©ner l'historique dans system ? Non, c'est moche.
                
                # Solution rapide: Utiliser Ollama pour le chat complexe pour l'instant
                # OU (Mieux) utiliser OpenRouter generate en mode "raw" si je l'avais expos√©.
                
                pass # TODO: Am√©liorer le support Chat OpenRouter
            except Exception:
                pass

        # Fallback Ollama (qui a une m√©thode chat native)
        return await self.ollama_client.chat(messages, **kwargs)
